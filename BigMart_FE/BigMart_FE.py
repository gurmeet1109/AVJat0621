'''
Problem Statement
------------------
Hi Everyone, The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. So now your aim is to build a predictive model and predict the sales of each product at a particular outlet.
Using this model, BigMart will try to understand the properties of products and outlets which play a key role in increasing sales.
Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.
Although we are attaching the required documents in the attached zip folder but You can also use the below mentioned link and register here to access the problem statement and dataset:

    https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/

The submission file generated by this code is attached in the same folder, Please use the solution checker tab to upload the submission file, the public score would be generated immediately.

Table of Content
    Step 1: Importing the Relevant Libraries
    Step 2: Data Inspection
    Step 3: Data Cleaning
    Step 4: Exploratory Data Analysis
    Step 5: Feature Engineering
    Step 6: Building Model

    How to Make a Submission?
    Guidelines for Final Submission

'''

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LinearRegression

import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# Setting Pandas display options for proper display without truncation
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', 1000)
pd.set_option('mode.chained_assignment', None)

# Reading input files
train = pd.read_csv('train_v9rqX0R.csv')
test = pd.read_csv('test_AbJTz2l.csv')

# ---- Data Inspection   -------------------
# Sample printing - To make sure data files are read correctly
print(train.head)
print(train.shape)

print(test.head)
print(train.columns.tolist())
print(test.shape)
print(test.columns.tolist())

# Ratio of null values
print(train.isnull().sum()/train.shape[0]*100)
print(test.isnull().sum()/test.shape[0]*100)

# Categorial and numerical features in train data
categorial = train.select_dtypes(include = [np.object])
print("Categorial Features in Train Data Set = ", categorial.shape[1])

numerical = train.select_dtypes(include = [np.float64, np.int64])
print("Numerical Features in Train Data Set = ", numerical.shape[1])

# Categorial and numerical features in Test data
categorial = test.select_dtypes(include = [np.object])
print("Categorial Features in Test Data Set = ", categorial.shape[1])

numerical = test.select_dtypes(include = [np.float64, np.int64])
print("Numerical Features in Test Data Set = ", numerical.shape[1])


# ------  Cleaning Data Files ------------

# Sum of null() by column names
print(train.isnull().sum())
print(test.isnull().sum())
print()

# Cleaning data for numerical value - 'Item_Weight'
# Checking for 'Outliners' by drawing box plots
plt.figure(figsize=(8, 5))
sns.boxplot('Item_Weight', data=train)
plt.savefig("train_Item_Weight.png")

plt.figure(figsize=(8, 5))
sns.boxplot('Item_Weight', data=test)
plt.savefig("test_Item_Weight.png")


# No Outlines mean we can substitute (Impute) with mean
# Impute with mean and checking for null values again
train['Item_Weight'] = train['Item_Weight'].fillna(train['Item_Weight'].mean())
test['Item_Weight'] = test['Item_Weight'].fillna(test['Item_Weight'].mean())
print("Null values now in train data = ", train['Item_Weight'].isnull().sum())
print("Null values now in test data =", test['Item_Weight'].isnull().sum())


# Cleaning data for categorial data 'Outlet_Size'
# Checking values
print(train.head(200))
print(train['Outlet_Size'].isnull().sum(), test['Outlet_Size'].isnull().sum())


print(train['Outlet_Size'].value_counts())
print('******************************************')
print(test['Outlet_Size'].value_counts())

# Since the outlet_size is a categorical column, we can impute the missing values by "Mode"(Most Repeated Value) from the column
#Imputing with Mode
train['Outlet_Size'] = train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0])
test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Size'].mode()[0])
print(train['Outlet_Size'].isnull().sum(), test['Outlet_Size'].isnull().sum())

print(train['Outlet_Size'].value_counts())
print('******************************************')
print(test['Outlet_Size'].value_counts())
print(train.head(200))



# Exploratory Data Analysis
print(train.columns)
train.head()

# Column name 'Item_Fat_Content' can be improved upon as it has got inconsistent data
print(train['Item_Fat_Content'].value_counts())

# Replacing multiple ambigous entries in 'Item_Fat_Content'
train['Item_Fat_Content'].replace(['low fat', 'LF', 'reg'], ['Low Fat', 'Low Fat', 'Regular'], inplace=True)
train['Item_Fat_Content'] = train['Item_Fat_Content'].astype(str)
print(train['Item_Fat_Content'].value_counts())
test['Item_Fat_Content'].replace(['low fat', 'LF', 'reg'], ['Low Fat', 'Low Fat', 'Regular'], inplace=True)
test['Item_Fat_Content'] = test['Item_Fat_Content'].astype(str)
print(test['Item_Fat_Content'].value_counts())

# Plots for train data

# Distribution plot according to fat content
plt.figure(figsize=(8, 5))
sns.countplot('Item_Fat_Content', data=train, palette='ocean')
plt.savefig("train_Distribution_FatContent.png")

# Distribution plot according to Item_Type
plt.figure(figsize=(25, 8))
sns.countplot('Item_Type', data=train, palette='rainbow')
plt.savefig("train_Distribution_ItemType.png")


# Distribution plot according to Outlet_Size
plt.figure(figsize=(8, 5))
sns.countplot('Outlet_Size', data=train, palette='summer')
plt.savefig("train_Distribution_OutletSize.png")

# Distribution plot according to Outlet_Location_Type
plt.figure(figsize=(8, 5))
sns.countplot('Outlet_Location_Type', data=train, palette='autumn')
plt.savefig("train_Distribution_OutletLocationType.png")

# Distribution plot according to Outlet_Type
plt.figure(figsize=(8, 5))
sns.countplot('Outlet_Type', data=train, palette='twilight')
plt.savefig("train_Distribution_OutletType.png")

# Distribution plot according to Item_Outlet_Sales
plt.figure(figsize=(10, 8))
sns.barplot(x='Item_Outlet_Sales', y='Item_Type', data=train, palette='flag')
plt.savefig("train_Distribution_ItemOutletSales.png")



# -------- Feature Engineering ----------------

print(train.head(5))
# Determine Years of Operation of store
train['Outlet_Years'] = 2021 - train['Outlet_Establishment_Year']
test['Outlet_Years'] = 2021 - test['Outlet_Establishment_Year']

# Create a broad category of type of item
train['Item_Type_Combined'] = train['Item_Identifier'].apply(lambda x: x[0:2])
train['Item_Type_Combined'] = train['Item_Type_Combined'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})
test['Item_Type_Combined'] = test['Item_Identifier'].apply(lambda x: x[0:2])
test['Item_Type_Combined'] = test['Item_Type_Combined'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})

# Modify Category of Item_Fat_Content
print(train['Item_Fat_Content'].value_counts())
train.loc[train['Item_Type_Combined'] == "Non-Consumable", 'Item_Fat_Content'] = "Non-Edible"
print(train['Item_Fat_Content'].value_counts())

print(test['Item_Fat_Content'].value_counts())
test.loc[test['Item_Type_Combined'] == "Non-Consumable", 'Item_Fat_Content'] = "Non-Edible"
print(test['Item_Fat_Content'].value_counts())

# --- One Hot Encoding ----
# Numerical and one hot coding of categorial variables
# Import library:
from sklearn.preprocessing import LabelEncoder

print("One Hot Encoding - Before Encoding")
print(train.head(5))
le = LabelEncoder()
# New variable for outlet - train data
train['Outlet'] = le.fit_transform(train['Outlet_Identifier'])
print("After OneHotEncoding of Outlet_Identifier")
print(train.head(5))
var_mod = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Item_Type_Combined', 'Outlet_Type', 'Outlet']

le = LabelEncoder()
for i in var_mod:
    train[i] = le.fit_transform(train[i])

# One Hot Coding:
train = pd.get_dummies(train, columns=['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Outlet_Type',
                                       'Item_Type_Combined', 'Outlet'])
print("After Encoding - train Data")
print(train.head(5))

# New variable for outlet - test data
test['Outlet'] = le.fit_transform(test['Outlet_Identifier'])
var_mod = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Item_Type_Combined', 'Outlet_Type', 'Outlet']
le = LabelEncoder()
for i in var_mod:
    test[i] = le.fit_transform(test[i])

# One Hot Coding:
test = pd.get_dummies(test, columns=['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Outlet_Type',
                                     'Item_Type_Combined', 'Outlet'])



#  ---------------- Building Model ----------------

# Removing all non-numeric columns
print(train.columns)
train = train.select_dtypes(exclude='object')
print(train.columns)
print(train.head(5))
test = test.select_dtypes(exclude='object')

# Separate features and target
X = train.drop(columns=['Item_Outlet_Sales'], axis=1)
y = train['Item_Outlet_Sales']

# 20% data as validation set
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=22)

'''
# Check if scaling is applicable using MinMaxScaler
# Refer https://medium.datadriveninvestor.com/predicting-credit-card-approvals-using-ml-techniques-9cd8eaeb5b8c
# Import MinMaxScaler. rescale X_train and X_valid using MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
rescaled_X_train = scaler.fit_transform(X_train)
rescaled_X_valid = scaler.fit_transform(X_valid)
'''


# Model Building
features = X.columns
LR = LinearRegression(normalize=True)
LR.fit(X_train, y_train)
y_pred = LR.predict(X_valid)
coef = pd.Series(LR.coef_, features).sort_values()

# Barplot for coefficients
plt.figure(figsize=(8,5))
sns.barplot(LR.coef_,features)
plt.savefig("Coef_LinearRegression.png")


# RMSE - Root Mean Squared error
MSE= metrics.mean_squared_error(y_valid, y_pred)
from math import sqrt
rmse = sqrt(MSE)
print("Root Mean Squared Error:", rmse)

'''
# Build Confusion Matrix and measure accuracy
from sklearn.metrics import confusion_matrix
print("Accuracy estimate ", LR.score(X_valid, y_valid))
print(confusion_matrix(y_valid, y_pred))
# ValueError: Classification metrics can't handle a mix of continuous and multiclass targets
'''

'''
# Use GridSearchCV and HyperParameterTuning to achieve best accuracy, as in
https://medium.datadriveninvestor.com/predicting-credit-card-approvals-using-ml-techniques-9cd8eaeb5b8c6

'''

# Generate Submission file
submission = pd.read_csv('sample_submission_8RXa3c6.csv')
final_predictions = LR.predict(test)
submission['Item_Outlet_Sales'] = final_predictions

#only positive predictions for the target variable
submission['Item_Outlet_Sales'] = submission['Item_Outlet_Sales'].apply(lambda x: 0 if x<0 else x)
submission.to_csv('my_submission.csv', index=False)
print(submission.head(10))

